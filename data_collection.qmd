---
title: Extracting Data - Everything
format: html
---
The purpose of this document is to extract the data from PubMed, using the R entrez API solution. Going to play around with parallelization, because it could speed things up greatly. Nevermind, pubmed does not like parallel.

Moreover, I have discovered that NCBI only allows 10,000 requests at a time, which means that we have to extract the data year by year. This is so sad.

```{r}
library(rentrez)
library(progressr)
library(data.table)
```

```{r}
# Set your Entrez API key
set_entrez_key("c14a6209a64f2dd809205225f00d76053007")

# Define query
query <- '("HIV" OR "AIDS") AND 
("epidemic" OR "treatment" OR "prevention" OR "detection" OR "vaccine" OR 
"mathematical modeling" OR "prognosis" OR "case study" OR "healthcare" OR 
"public health" OR "immunology" OR "infectious diseases") 
NOT ("conference" OR "summit" OR "legal" OR "audiology" OR "non-HIV infections" 
OR "qi gong" OR "hearing" OR "strategy" OR "debate" OR "response" OR "comments" OR "editorial")'

```

```{r}
# Define year ranges
start_year <- 2010
end_year <- 2024
#end_year <- 2002

# Define proper 5-year intervals (now 1 year intervals)
year_ranges <- lapply(seq(start_year, end_year, by = 1), function(year) {
  range_end <- min(year, end_year) # Ensure we don't exceed end_year
  return(c(year, range_end))
})



# Initialize progress bar
handlers(global = TRUE)
handlers("txtprogressbar") 
```

```{r}

# Function to fetch and save articles for a year range
fetch_and_save_articles <- function(range_start, range_end) {
  cat(sprintf("Processing range: %d-%d\n", range_start, range_end))
  
  # Perform the PubMed search
  search_results <- tryCatch(
    entrez_search(
      db = "pubmed",
      term = query,
      mindate = as.character(range_start),
      maxdate = as.character(range_end),
      use_history = TRUE
    ),
    error = function(e) {
      message(sprintf("Error during search for range %d-%d: %s", range_start, range_end, e$message))
      return(NULL)
    }
  )
  
  if (is.null(search_results) || search_results$count == 0) {
    message(sprintf("No results found for range %d-%d.", range_start, range_end))
    return(NULL)
  }
  
  cat(sprintf("Found %d results for range %d-%d\n", search_results$count, range_start, range_end))
  
  # Fetch article summaries in batches
  batch_size <- 100
  range_articles <- list()
  
  for (start_idx in seq(0, search_results$count - 1, by = batch_size)) {
    Sys.sleep(0.34)  # Rate limit: 3 requests per second
    summary_data <- tryCatch(
      entrez_summary(
        db = "pubmed",
        web_history = search_results$web_history,
        retmax = batch_size,
        retstart = start_idx
      ),
      error = function(e) {
        message(sprintf("Error during summary fetch for batch starting at %d: %s", start_idx, e$message))
        return(NULL)
      }
    )
    #str(summary_data)
    if (is.null(summary_data)) next
    
    # Extract and validate data
    articles <- lapply(summary_data, function(x) {
      if (is.atomic(x)) {
        message("Skipping atomic vector in summary data.")
        return(NULL)
      }
      if (!is.null(x$uid) && !is.null(x$sorttitle) && !is.null(x$pubdate)) {
        pub_year <- as.numeric(sub(".*?([0-9]{4}).*", "\\1", x$pubdate))
        return(list(
          UID = x$uid,
          Title = x$sorttitle,
          PublicationDate = x$pubdate,
          PublicationYear = pub_year
        ))
      }
      NULL
    })


    # Filter out NULL entries
    articles <- Filter(Negate(is.null), articles)
    # Combine into a data frame or data table
    if (length(articles) > 0) {
      batch_df <- data.table::rbindlist(articles, use.names = TRUE, fill = TRUE)
      range_articles[[as.character(start_idx)]] <- batch_df
    } else {
      message(sprintf("No valid articles in batch starting at %d", start_idx))
    }

  }

  if (length(range_articles) > 0) {
    range_df <- data.table::rbindlist(range_articles, use.names = TRUE, fill = TRUE)
    range_df <- unique(range_df)  # Deduplicate
    cat("there are ", nrow(range_df), "articles for this range\n")
    file_name <- sprintf("data/pubmed_hiv_articles_%d_%d.csv", range_start, range_end)
    data.table::fwrite(range_df, file_name)  # Write to CSV
    cat(sprintf("Saved articles for range %d-%d to %s\n", range_start, range_end, file_name))
  } else {
    message(sprintf("No articles to save for range %d-%d.", range_start, range_end))
  }

}

```


```{r}

# Main processing loop with progress bar
progressr::with_progress({
  p <- progressr::progressor(along = year_ranges)
  for (range in year_ranges) {
    p(message = sprintf("Processing range %d-%d", range[1], range[2]))
    fetch_and_save_articles(range[1], range[2])
  }
})

```
