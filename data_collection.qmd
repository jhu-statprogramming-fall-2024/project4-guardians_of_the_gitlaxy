---
title: Extracting Data - Everything
format: html
---
The purpose of this document is to extract the data from PubMed, using the R entrez API solution. Going to play around with parallelization, because it could speed things up greatly. Nevermind, pubmed does not like parallel

```{r}
library(rentrez)
library(progressr)
```

```{r}
# Set your Entrez API key
set_entrez_key("c14a6209a64f2dd809205225f00d76053007")

# Define query
query <- '("HIV" OR "AIDS") AND 
("epidemic" OR "treatment" OR "prevention" OR "detection" OR "vaccine" OR 
"mathematical modeling" OR "prognosis" OR "case study" OR "healthcare" OR 
"public health" OR "immunology" OR "infectious diseases") 
NOT ("conference" OR "summit" OR "legal" OR "audiology" OR "non-HIV infections" 
OR "qi gong" OR "hearing" OR "strategy" OR "debate" OR "response" OR "comments" OR "editorial")'

```

```{r}
# Define year ranges
start_year <- 1990
end_year <- as.integer(format(Sys.Date(), "%Y"))
year_ranges <- split(seq(start_year, end_year, by = 5), 
                     ceiling(seq_along(seq(start_year, end_year, by = 5)) / 1))


# Initialize progress bar
handlers(global = TRUE)
handlers("txtprogressbar") 
```

```{r}


progressr::with_progress({
  p <- progressr::progressor(along = year_ranges)
  
  for (range_start in year_ranges) {
    range_end <- min(range_start + 4, end_year)
    p(message = sprintf("Processing range: %d-%d", range_start, range_end))
    
    # Search for articles in the range
    search_results <- tryCatch(
      entrez_search(
        db = "pubmed",
        term = query,
        mindate = as.character(range_start),
        maxdate = as.character(range_end),
        use_history = TRUE
      ),
      error = function(e) {
        cat("Error during search:", e$message, "\n")
        return(NULL)
      }
    )
    
    if (is.null(search_results) || search_results$count == 0) {
      next  # Skip this range if no results or an error occurred
    }
    
    cat('Number of search results:', search_results$count, '\n')
    range_articles <- list()
    batch_size <- 100
    
    for (start_idx in seq(0, search_results$count - 1, by = batch_size)) {
      Sys.sleep(0.1)  # Ensure at most 3 requests per second
      
      summary_data <- tryCatch(
        entrez_summary(
          db = "pubmed",
          web_history = search_results$web_history,
          retmax = batch_size,
          retstart = start_idx
        ),
        error = function(e) {
          cat("Error during summary fetch:", e$message, "\n")
          return(NULL)
        }
      )
      
      if (is.null(summary_data)) {
        next
      }
      
      articles <- lapply(summary_data, function(x) c(x$uid, x$sorttitle, x$pubdate))
      batch_df <- do.call(rbind, articles)
      batch_df <- data.frame(batch_df, stringsAsFactors = FALSE)
      colnames(batch_df) <- c("UID", "Title", "PublicationDate")
      
      range_articles[[paste0(start_idx)]] <- batch_df
    }
    
    # Combine batches for the range and save as CSV
    if (length(range_articles) > 0) {
      range_df <- do.call(rbind, range_articles)
      file_name <- paste0("data/pubmed_hiv_articles_", range_start, "_", range_end, ".csv")
      write.csv(range_df, file_name, row.names = FALSE)
      cat("Saved articles for range", range_start, "-", range_end, "to", file_name, "\n")
    }
  }
})

```
